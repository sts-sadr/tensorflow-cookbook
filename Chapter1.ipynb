{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Getting Started with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Tensors\n",
    "\n",
    "### 1.1 Fixed tensors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dim = 4\n",
    "col_dim = 5\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a zero filled tensor. Use the following:\n",
    "zero_tsr = tf.zeros([row_dim, col_dim])\n",
    "print(zero_tsr)\n",
    "sess.run(zero_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a one filled tensor. Use the following: \n",
    "ones_tsr = tf.ones([row_dim, col_dim])\n",
    "print(ones_tsr)\n",
    "sess.run(ones_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Fill:0\", shape=(4, 5), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[42, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a constant filled tensor. Use the following: \n",
    "filled_tsr = tf.fill([row_dim, col_dim], 42)\n",
    "print(filled_tsr)\n",
    "sess.run(filled_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a tensor out of an existing constant. Use the following:\n",
    "constant_tsr = tf.constant([1,2,3])\n",
    "print(constant_tsr)\n",
    "sess.run(constant_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensors of similar shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_like:0\", shape=(3,), dtype=int32) \n",
      " Tensor(\"ones_like:0\", shape=(3,), dtype=int32)\n",
      "zeros_similar:  [0 0 0]\n",
      "ones_similar:  [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#We can also initialize variables based on the shape of other tensors, as follows:\n",
    "zeros_similar = tf.zeros_like(constant_tsr)\n",
    "ones_similar = tf.ones_like(constant_tsr)\n",
    "print(zeros_similar,'\\n', ones_similar)\n",
    "print('zeros_similar: ', sess.run(zeros_similar))\n",
    "print('ones_similar: ', sess.run(ones_similar))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sequence tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LinSpace:0\", shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 1. ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow allows us to specify tensors that contain defined intervals. The following functions behave very \n",
    "# similarly to the range() outputs and numpy's linspace() outputs. See the following function:\n",
    "linear_tsr = tf.linspace(start=0.0, stop=1, num=3)\n",
    "print(linear_tsr)\n",
    "sess.run(linear_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"range:0\", shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6,  9, 12], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The resulting tensor is the sequence [0.0, 0.5, 1.0]. \n",
    "# Note that this function includes the specified stop value. See the following function:\n",
    "# The result is the sequence [6, 9, 12]. Note that this function does not include the limit value.\n",
    "integer_seq_tsr = tf.range(start=6, limit=15, delta=3)\n",
    "print(integer_seq_tsr)\n",
    "sess.run(integer_seq_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Random tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_uniform:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.11488271, 0.07902026, 0.591535  , 0.34673882, 0.38580537],\n",
       "       [0.76593566, 0.7253599 , 0.14558256, 0.40344012, 0.42261922],\n",
       "       [0.45649695, 0.3998834 , 0.41987538, 0.9466213 , 0.66327965],\n",
       "       [0.8034619 , 0.05617726, 0.45696282, 0.35578716, 0.22598755]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that this random uniform distribution draws from the interval \n",
    "# that includes the minval but not the maxval (minval <= x < maxval).\n",
    "randunif_tsr = tf.random_uniform([row_dim, col_dim], minval=0, maxval=1)\n",
    "print(randunif_tsr)\n",
    "sess.run(randunif_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.51794595, -1.9094315 , -2.7199473 , -1.6990983 , -1.5759071 ],\n",
       "       [-0.77356356, -1.6158706 , -0.65148175, -0.03875288,  0.90010357],\n",
       "       [-0.38554174, -1.2360102 ,  1.7376286 ,  0.32748318,  0.898134  ],\n",
       "       [-1.272865  ,  1.0944372 ,  1.5664771 , -0.63088495, -0.00801061]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get a tensor with random draws from a normal distribution, as follows:\n",
    "randnorm_tsr = tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0)\n",
    "print(randnorm_tsr)\n",
    "sess.run(randnorm_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truncated_normal:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.82263833,  0.28221107,  0.5995017 , -0.4973465 , -0.08138938],\n",
       "       [-0.62193006, -0.7295433 ,  1.4328749 , -0.5045136 ,  1.0136473 ],\n",
       "       [-0.94997764, -0.6004379 ,  1.5189623 , -0.33910874, -0.9147056 ],\n",
       "       [-0.586182  ,  0.9479407 ,  0.607719  ,  0.15074769,  0.07470734]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are also times when we wish to generate normal random values that are assured within certain bounds.\n",
    "# The truncated_normal() function always picks normal values within two standard deviations of the specified mean.\n",
    "# See the following:\n",
    "runcnorm_tsr = tf.truncated_normal([row_dim, col_dim], mean=0.0, stddev=1.0)\n",
    "print(runcnorm_tsr)\n",
    "sess.run(runcnorm_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We might also be interested in randomizing entries of arrays. To accomplish this, there are two functions that help us: ```random_shuffle()``` and ``random_crop()``. See the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_output = tf.random_shuffle(runcnorm_tsr)\n",
    "# cropped_output = tf.random_crop(runcnorm_tsr, 2)\n",
    "\n",
    "# print('shuffled_output: ', shuffled_output, sess.run(shuffled_output))\n",
    "# print('cropped_output: ', cropped_output,sess.run(cropped_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on in this book, we will be interested in randomly cropping an image of size (height, width, 3) where there are three color spectrums. To fix a dimension in the cropped_output, you must give it the maximum size in that dimension:\n",
    "#cropped_image = tf.random_crop(my_image, [height/2, width/2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Placeholders and Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders and variables are key tools for using computational graphs in TensorFlow. We must understand the difference and when to best use them to our advantage.\n",
    "### Getting ready\n",
    "One of the most important distinctions to make with the data is whether it is a placeholder or a variable. Variables are the parameters of the algorithm and TensorFlow keeps track of how to change these to optimize the algorithm. Placeholders are objects that allow you to feed in data of a specific type and shape and depend on the results of the computational graph, such as the expected outcome of a computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_var = tf.Variable(tf.zeros([2,3]))\n",
    "sess = tf.Session()\n",
    "initialize_op = tf.global_variables_initializer ()\n",
    "sess.run(initialize_op)\n",
    "sess.run(my_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24112868, 0.8958062 ],\n",
       "       [0.812914  , 0.93434095]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[2,2])\n",
    "y = tf.identity(x)\n",
    "x_vals = np.random.rand(2,2)\n",
    "sess.run(y, feed_dict={x: x_vals})\n",
    "# Note that sess.run(x, feed_dict={x: x_vals}) will result in a self-referencing error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the run of the computational graph, we have to tell TensorFlow when to initialize the variables we have created. TensorFlow must be informed about when it can initialize the variables. While each variable has an initializer method, the most common way to do this is to use the helper function, which is `global_variables_initializer()`.\n",
    "This function creates an operation in the graph that initializes all the variables we have created, as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer_op = tf.global_variables_initializer()\n",
    "sess.run(initializer_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we want to initialize a variable based on the results of initializing another variable, we have to initialize variables in the order we want, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "first_var = tf.Variable(tf.zeros([2,3]))\n",
    "sess.run(first_var.initializer)\n",
    "second_var = tf.Variable(tf.zeros_like(first_var))\n",
    "# Depends on first_var\n",
    "sess.run(second_var.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating matrices: We can create two-dimensional matrices from numpy arrays or nested lists, as we described in the earlier section on tensors. We can also use the tensor creation functions and specify a two-dimensional shape for functions such as `zeros()`, `ones()`, `truncated_normal()`, and so on. TensorFlow also allows us to create a diagonal matrix from a one-dimensional array or list with the function `diag()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity_matrix: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "A: \n",
      " [[ 1.740125   0.3309737  0.6038319]\n",
      " [-1.133956  -0.333832  -1.642118 ]]\n",
      "B: \n",
      " [[5. 5. 5.]\n",
      " [5. 5. 5.]]\n",
      "C: \n",
      " [[0.92835104 0.30271435]\n",
      " [0.34088457 0.19518423]\n",
      " [0.7691642  0.11188972]]\n",
      "D: \n",
      " [[ 1.  2.  3.]\n",
      " [-3. -7. -1.]\n",
      " [ 0.  5. -2.]]\n"
     ]
    }
   ],
   "source": [
    "identity_matrix = tf.diag([1.0, 1.0, 1.0])\n",
    "A = tf.truncated_normal([2,3])\n",
    "B = tf.fill([2,3], 5.0)\n",
    "C = tf.random_uniform([3,2])\n",
    "D = tf.convert_to_tensor(np.array([[1., 2., 3.],[-3., -7., -1.],[0., 5., -2.]]))\n",
    "print('identity_matrix: \\n', sess.run(identity_matrix))\n",
    "print('A: \\n', sess.run(A))\n",
    "print('B: \\n', sess.run(B))\n",
    "print('C: \\n', sess.run(C))\n",
    "print('D: \\n', sess.run(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9817467 3.6687613 4.7442594]\n",
      " [5.3625154 5.2944193 3.5542712]]\n"
     ]
    }
   ],
   "source": [
    "# Addition and subtraction uses the following function: \n",
    "print(sess.run(A+B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(B-B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 5. 5.]\n",
      " [5. 5. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# Also, the function matmul() has arguments that specify whether or not to transpose\n",
    "# the arguments before multiplication or whether each matrix is sparse.\n",
    "\n",
    "print(sess.run(tf.matmul(B, identity_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98711777 0.3272189  0.6695057 ]\n",
      " [0.7447648  0.50297153 0.6781281 ]]\n"
     ]
    }
   ],
   "source": [
    "# Transpose the arguments as follows:\n",
    "print(sess.run(tf.transpose(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.99999999999999\n"
     ]
    }
   ],
   "source": [
    "# For the determinant, use the following:\n",
    "print(sess.run(tf.matrix_determinant(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5        -0.5        -0.5       ]\n",
      " [ 0.15789474  0.05263158  0.21052632]\n",
      " [ 0.39473684  0.13157895  0.02631579]]\n"
     ]
    }
   ],
   "source": [
    "# Inverse\n",
    "print(sess.run(tf.matrix_inverse(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# For the Cholesky decomposition, use the following:\n",
    "print(sess.run(tf.cholesky(identity_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-10.65907521,  -0.22750691,   2.88658212]), array([[ 0.21749542,  0.63250104, -0.74339638],\n",
      "       [ 0.84526515,  0.2587998 ,  0.46749277],\n",
      "       [-0.4880805 ,  0.73004459,  0.47834331]]))\n"
     ]
    }
   ],
   "source": [
    "# For Eigenvalues and eigenvectors, use the following code:\n",
    "print(sess.run(tf.self_adjoint_eig(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.div(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.truediv(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# If we have floats and want an integer division\n",
    "print(sess.run(tf.floordiv(3.0,4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.mod(22.0, 5.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# The cross-product between two tensors is achieved\n",
    "print(sess.run(tf.cross([1., 0., 0.], [0., 1., 0.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000036\n"
     ]
    }
   ],
   "source": [
    "# Tangent function (tan(pi/4)=1)\n",
    "print(sess.run(tf.div(tf.sin(3.1416/4.), tf.cos(3.1416/4.))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "def custom_polynomial(value):\n",
    "    return(tf.subtract(3 * tf.square(value), value) + 10)\n",
    "print(sess.run(custom_polynomial(11)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  3. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.nn.relu([-3., 3., 10.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 6.]\n"
     ]
    }
   ],
   "source": [
    "#The implementation that TensorFlow has is called the ReLU6 function. This is defined as min(max(0,x),6). \n",
    "# This is a version of the hard- sigmoid function and is computationally faster, and does not suffer from \n",
    "# vanishing (infinitesimally near zero) or exploding values. \n",
    "print(sess.run(tf.nn.relu6([-3., 3., 10.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26894143 0.5        0.7310586 ]\n"
     ]
    }
   ],
   "source": [
    "# The sigmoid function is the most common continuous and smooth activation function. \n",
    "# It is also called a logistic function and has the form 1/(1+exp(-x)). \n",
    "print(sess.run(tf.nn.sigmoid([-1., 0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7615942  0.         0.7615942]\n"
     ]
    }
   ],
   "source": [
    "# The hyper tangent function is very similar to the sigmoid except that instead of having a range \n",
    "# between 0 and 1, it has a range between -1 and 1. \n",
    "print(sess.run(tf.nn.tanh([-1., 0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  0.  -0.5]\n"
     ]
    }
   ],
   "source": [
    "# The form of this function is x/(abs(x) + 1). \n",
    "print(sess.run(tf.nn.softsign([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31326166 0.6931472  0.31326166]\n"
     ]
    }
   ],
   "source": [
    "# Another function, the softplus, is a smooth version of the ReLU function. The form of this \n",
    "# function is log(exp(x) + 1).\n",
    "print(sess.run(tf.nn.softplus([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Exponential Linear Unit (ELU) is very similar to the softplus function except that the bottom \n",
    "# asymptote is -1 instead of 0. The form is (exp(x)+1) if x < 0 else x.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Getting Started with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Tensors\n",
    "\n",
    "### 1.1 Fixed tensors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dim = 4\n",
    "col_dim = 5\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a zero filled tensor. Use the following:\n",
    "zero_tsr = tf.zeros([row_dim, col_dim])\n",
    "print(zero_tsr)\n",
    "sess.run(zero_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a one filled tensor. Use the following: \n",
    "ones_tsr = tf.ones([row_dim, col_dim])\n",
    "print(ones_tsr)\n",
    "sess.run(ones_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Fill:0\", shape=(4, 5), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[42, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42],\n",
       "       [42, 42, 42, 42, 42]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a constant filled tensor. Use the following: \n",
    "filled_tsr = tf.fill([row_dim, col_dim], 42)\n",
    "print(filled_tsr)\n",
    "sess.run(filled_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a tensor out of an existing constant. Use the following:\n",
    "constant_tsr = tf.constant([1,2,3])\n",
    "print(constant_tsr)\n",
    "sess.run(constant_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensors of similar shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_like:0\", shape=(3,), dtype=int32) \n",
      " Tensor(\"ones_like:0\", shape=(3,), dtype=int32)\n",
      "zeros_similar:  [0 0 0]\n",
      "ones_similar:  [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#We can also initialize variables based on the shape of other tensors, as follows:\n",
    "zeros_similar = tf.zeros_like(constant_tsr)\n",
    "ones_similar = tf.ones_like(constant_tsr)\n",
    "print(zeros_similar,'\\n', ones_similar)\n",
    "print('zeros_similar: ', sess.run(zeros_similar))\n",
    "print('ones_similar: ', sess.run(ones_similar))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sequence tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LinSpace:0\", shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 1. ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow allows us to specify tensors that contain defined intervals. The following functions behave very \n",
    "# similarly to the range() outputs and numpy's linspace() outputs. See the following function:\n",
    "linear_tsr = tf.linspace(start=0.0, stop=1, num=3)\n",
    "print(linear_tsr)\n",
    "sess.run(linear_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"range:0\", shape=(3,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6,  9, 12], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The resulting tensor is the sequence [0.0, 0.5, 1.0]. \n",
    "# Note that this function includes the specified stop value. See the following function:\n",
    "# The result is the sequence [6, 9, 12]. Note that this function does not include the limit value.\n",
    "integer_seq_tsr = tf.range(start=6, limit=15, delta=3)\n",
    "print(integer_seq_tsr)\n",
    "sess.run(integer_seq_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Random tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_uniform:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7912855 , 0.4553269 , 0.9440459 , 0.18774247, 0.6759523 ],\n",
       "       [0.76959527, 0.60909295, 0.42014956, 0.51050353, 0.19272888],\n",
       "       [0.12932575, 0.71849   , 0.02474761, 0.6868391 , 0.59169483],\n",
       "       [0.4085058 , 0.9336953 , 0.4273454 , 0.8807064 , 0.7339244 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that this random uniform distribution draws from the interval \n",
    "# that includes the minval but not the maxval (minval <= x < maxval).\n",
    "randunif_tsr = tf.random_uniform([row_dim, col_dim], minval=0, maxval=1)\n",
    "print(randunif_tsr)\n",
    "sess.run(randunif_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.27968043,  1.8213042 ,  0.7313014 ,  1.156326  , -0.69455653],\n",
       "       [-0.51916957, -1.174872  , -0.88730067, -0.22928411, -0.8486738 ],\n",
       "       [ 0.5022169 , -0.88140327,  0.710257  , -0.03452583, -0.722225  ],\n",
       "       [-0.15739939, -0.3070296 ,  0.58209103, -0.5447573 ,  0.7729796 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get a tensor with random draws from a normal distribution, as follows:\n",
    "randnorm_tsr = tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0)\n",
    "print(randnorm_tsr)\n",
    "sess.run(randnorm_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truncated_normal:0\", shape=(4, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.22587982, -0.7041513 , -1.6064415 , -1.121558  ,  0.73343486],\n",
       "       [-0.54530483,  1.0190886 ,  1.3497546 , -0.2436367 ,  0.5042937 ],\n",
       "       [ 0.834888  , -0.10719744, -1.6145267 , -1.2179449 ,  0.21531558],\n",
       "       [ 1.1177862 ,  1.1505157 ,  0.4790224 , -0.63684785,  1.9048425 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are also times when we wish to generate normal random values that are assured within certain bounds.\n",
    "# The truncated_normal() function always picks normal values within two standard deviations of the specified mean.\n",
    "# See the following:\n",
    "runcnorm_tsr = tf.truncated_normal([row_dim, col_dim], mean=0.0, stddev=1.0)\n",
    "print(runcnorm_tsr)\n",
    "sess.run(runcnorm_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We might also be interested in randomizing entries of arrays. To accomplish this, there are two functions that help us: ```random_shuffle()``` and ``random_crop()``. See the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_output = tf.random_shuffle(runcnorm_tsr)\n",
    "# cropped_output = tf.random_crop(runcnorm_tsr, 2)\n",
    "\n",
    "# print('shuffled_output: ', shuffled_output, sess.run(shuffled_output))\n",
    "# print('cropped_output: ', cropped_output,sess.run(cropped_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on in this book, we will be interested in randomly cropping an image of size (height, width, 3) where there are three color spectrums. To fix a dimension in the cropped_output, you must give it the maximum size in that dimension:\n",
    "#cropped_image = tf.random_crop(my_image, [height/2, width/2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Placeholders and Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders and variables are key tools for using computational graphs in TensorFlow. We must understand the difference and when to best use them to our advantage.\n",
    "### Getting ready\n",
    "One of the most important distinctions to make with the data is whether it is a placeholder or a variable. Variables are the parameters of the algorithm and TensorFlow keeps track of how to change these to optimize the algorithm. Placeholders are objects that allow you to feed in data of a specific type and shape and depend on the results of the computational graph, such as the expected outcome of a computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_var = tf.Variable(tf.zeros([2,3]))\n",
    "sess = tf.Session()\n",
    "initialize_op = tf.global_variables_initializer ()\n",
    "sess.run(initialize_op)\n",
    "sess.run(my_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92516834, 0.7077631 ],\n",
       "       [0.72848994, 0.38107234]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[2,2])\n",
    "y = tf.identity(x)\n",
    "x_vals = np.random.rand(2,2)\n",
    "sess.run(y, feed_dict={x: x_vals})\n",
    "# Note that sess.run(x, feed_dict={x: x_vals}) will result in a self-referencing error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the run of the computational graph, we have to tell TensorFlow when to initialize the variables we have created. TensorFlow must be informed about when it can initialize the variables. While each variable has an initializer method, the most common way to do this is to use the helper function, which is `global_variables_initializer()`.\n",
    "This function creates an operation in the graph that initializes all the variables we have created, as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer_op = tf.global_variables_initializer()\n",
    "sess.run(initializer_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we want to initialize a variable based on the results of initializing another variable, we have to initialize variables in the order we want, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "first_var = tf.Variable(tf.zeros([2,3]))\n",
    "sess.run(first_var.initializer)\n",
    "second_var = tf.Variable(tf.zeros_like(first_var))\n",
    "# Depends on first_var\n",
    "sess.run(second_var.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating matrices: We can create two-dimensional matrices from numpy arrays or nested lists, as we described in the earlier section on tensors. We can also use the tensor creation functions and specify a two-dimensional shape for functions such as `zeros()`, `ones()`, `truncated_normal()`, and so on. TensorFlow also allows us to create a diagonal matrix from a one-dimensional array or list with the function `diag()`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity_matrix: \n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "A: \n",
      " [[ 0.41711855 -0.7454542  -0.11365662]\n",
      " [-0.16381666  0.33388212  0.18860054]]\n",
      "B: \n",
      " [[5. 5. 5.]\n",
      " [5. 5. 5.]]\n",
      "C: \n",
      " [[0.85718024 0.1441617 ]\n",
      " [0.01697803 0.9305912 ]\n",
      " [0.356102   0.19079506]]\n",
      "D: \n",
      " [[ 1.  2.  3.]\n",
      " [-3. -7. -1.]\n",
      " [ 0.  5. -2.]]\n"
     ]
    }
   ],
   "source": [
    "identity_matrix = tf.diag([1.0, 1.0, 1.0])\n",
    "A = tf.truncated_normal([2,3])\n",
    "B = tf.fill([2,3], 5.0)\n",
    "C = tf.random_uniform([3,2])\n",
    "D = tf.convert_to_tensor(np.array([[1., 2., 3.],[-3., -7., -1.],[0., 5., -2.]]))\n",
    "print('identity_matrix: \\n', sess.run(identity_matrix))\n",
    "print('A: \\n', sess.run(A))\n",
    "print('B: \\n', sess.run(B))\n",
    "print('C: \\n', sess.run(C))\n",
    "print('D: \\n', sess.run(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9937425 5.021853  3.2444577]\n",
      " [4.5673265 3.3217802 6.2834606]]\n"
     ]
    }
   ],
   "source": [
    "# Addition and subtraction uses the following function: \n",
    "print(sess.run(A+B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(B-B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 5. 5.]\n",
      " [5. 5. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# Also, the function matmul() has arguments that specify whether or not to transpose\n",
    "# the arguments before multiplication or whether each matrix is sparse.\n",
    "\n",
    "print(sess.run(tf.matmul(B, identity_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.91107595 0.45836318 0.54793143]\n",
      " [0.24193418 0.35453534 0.25038493]]\n"
     ]
    }
   ],
   "source": [
    "# Transpose the arguments as follows:\n",
    "print(sess.run(tf.transpose(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.99999999999999\n"
     ]
    }
   ],
   "source": [
    "# For the determinant, use the following:\n",
    "print(sess.run(tf.matrix_determinant(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5        -0.5        -0.5       ]\n",
      " [ 0.15789474  0.05263158  0.21052632]\n",
      " [ 0.39473684  0.13157895  0.02631579]]\n"
     ]
    }
   ],
   "source": [
    "# Inverse\n",
    "print(sess.run(tf.matrix_inverse(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# For the Cholesky decomposition, use the following:\n",
    "print(sess.run(tf.cholesky(identity_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-10.65907521,  -0.22750691,   2.88658212]), array([[ 0.21749542,  0.63250104, -0.74339638],\n",
      "       [ 0.84526515,  0.2587998 ,  0.46749277],\n",
      "       [-0.4880805 ,  0.73004459,  0.47834331]]))\n"
     ]
    }
   ],
   "source": [
    "# For Eigenvalues and eigenvectors, use the following code:\n",
    "print(sess.run(tf.self_adjoint_eig(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.div(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.truediv(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# If we have floats and want an integer division\n",
    "print(sess.run(tf.floordiv(3.0,4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.mod(22.0, 5.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# The cross-product between two tensors is achieved\n",
    "print(sess.run(tf.cross([1., 0., 0.], [0., 1., 0.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000036\n"
     ]
    }
   ],
   "source": [
    "# Tangent function (tan(pi/4)=1)\n",
    "print(sess.run(tf.div(tf.sin(3.1416/4.), tf.cos(3.1416/4.))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "def custom_polynomial(value):\n",
    "    return(tf.subtract(3 * tf.square(value), value) + 10)\n",
    "print(sess.run(custom_polynomial(11)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  3. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.nn.relu([-3., 3., 10.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 6.]\n"
     ]
    }
   ],
   "source": [
    "#The implementation that TensorFlow has is called the ReLU6 function. This is defined as min(max(0,x),6). \n",
    "# This is a version of the hard- sigmoid function and is computationally faster, and does not suffer from \n",
    "# vanishing (infinitesimally near zero) or exploding values. \n",
    "print(sess.run(tf.nn.relu6([-3., 3., 10.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26894143 0.5        0.7310586 ]\n"
     ]
    }
   ],
   "source": [
    "# The sigmoid function is the most common continuous and smooth activation function. \n",
    "# It is also called a logistic function and has the form 1/(1+exp(-x)). \n",
    "print(sess.run(tf.nn.sigmoid([-1., 0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7615942  0.         0.7615942]\n"
     ]
    }
   ],
   "source": [
    "# The hyper tangent function is very similar to the sigmoid except that instead of having a range \n",
    "# between 0 and 1, it has a range between -1 and 1. \n",
    "print(sess.run(tf.nn.tanh([-1., 0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  0.  -0.5]\n"
     ]
    }
   ],
   "source": [
    "# The form of this function is x/(abs(x) + 1). \n",
    "print(sess.run(tf.nn.softsign([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31326166 0.6931472  0.31326166]\n"
     ]
    }
   ],
   "source": [
    "# Another function, the softplus, is a smooth version of the ReLU function. The form of this \n",
    "# function is log(exp(x) + 1).\n",
    "print(sess.run(tf.nn.softplus([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63212055  0.         -0.63212055]\n"
     ]
    }
   ],
   "source": [
    "# The Exponential Linear Unit (ELU) is very similar to the softplus function except that the bottom \n",
    "# asymptote is -1 instead of 0. The form is (exp(x)+1) if x < 0 else x.\n",
    "print(sess.run(tf.nn.elu([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data Sources\n",
    "\n",
    "**Iris data:** This dataset is arguably the most classic dataset used in machine learning and maybe all of statistics. It is a dataset that measures sepal length, sepal width, petal length, and petal width of three different types of iris flowers: Iris setosa, Iris virginica, and Iris versicolor. There are 150 measurements overall, 50 measurements of each species. To load the dataset in Python, we use Scikit Learn's dataset function, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "len(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data[0] # Sepal length, Sepal width, Petal length, Petal width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Birth weight data:** The University of Massachusetts at Amherst has compiled many statistical datasets that are of interest (1). One such dataset is a measure of child birth weight and other demographic and medical measurements of the mother and family history. There are 189 observations of 11 variables. Here is how to access the data in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "birthdata_url = 'https://github.com/nfmcclure/tensorflow_cookbook/raw/master/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat'\n",
    "birth_file = requests.get(birthdata_url)\n",
    "birth_data = birth_file.text.split('\\r\\n')\n",
    "birth_header = birth_data[0].split('\\t')\n",
    "birth_data = [[float(x) for x in y.split('\\t') if len(x)>=1] for y in birth_data[1:] if len(y)>=1]\n",
    "print(len(birth_data))\n",
    "print(len(birth_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boston Housing data:** Carnegie Mellon University maintains a library of datasets in their Statlib Library. This data is easily accessible via The University of California at Irvine's Machine-Learning Repository (2). There are 506 observations of house worth along with various demographic data and housing attributes (14 variables). Here is how to access the data in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57344/57026 [==============================] - 0s 6us/step\n",
      "404\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# Housing Price Data\n",
    "from keras.datasets import boston_housing\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "housing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "print(x_train.shape[0])\n",
    "print(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST handwriting data:** MNIST (Mixed National Institute of Standards and Technology) is a subset of the larger NIST handwriting database. The MNIST handwriting dataset is hosted on Yann LeCun's website (https://yann.lecun. com/exdb/mnist/). It is a database of 70,000 images of single digit numbers (0-9) with about 60,000 annotated for a training set and 10,000 for a test set. This dataset is used so often in image recognition that TensorFlow provides built-in functions to access this data. In machine learning, it is also important to provide validation data to prevent overfitting (target leakage). Because of this TensorFlow, sets aside 5,000 of the train set into a validation set. Here is how to access the data in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-4b14afc24f98>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Library/Anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Library/Anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Library/Anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "55000\n",
      "10000\n",
      "5000\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(len(mnist.train.images))\n",
    "print(len(mnist.test.images))\n",
    "print(len(mnist.validation.images))\n",
    "print(mnist.train.labels[1,:]) # The first label is a 3'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spam-ham text data:** UCI's machine -learning data set library (2) also holds a spam- ham text message dataset. We can access this .zip file and get the spam-ham text data as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "{'spam', 'ham'}\n",
      "Ok lar... Joking wif u oni...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(zip_url)\n",
    "z = ZipFile(io.BytesIO(r.content))\n",
    "file = z.read('SMSSpamCollection')\n",
    "text_data = file.decode()\n",
    "text_data = text_data.encode('ascii',errors='ignore')\n",
    "text_data = text_data.decode().split('\\n')\n",
    "text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\n",
    "print(len(text_data_train))\n",
    "print(set(text_data_target))\n",
    "print(text_data_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movie review data:** Bo Pang from Cornell has released a movie review dataset that classifies reviews as good or bad (3). You can find the data on the website, http:// www.cs.cornell.edu/people/pabo/movie-review-data/. To download, extract, and transform this data, we run the following code:\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n",
      "simplistic , silly and tedious . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import tarfile\n",
    "movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "r = requests.get(movie_data_url)\n",
    "# Stream data into temp object\n",
    "stream_data = io.BytesIO(r.content)\n",
    "tmp = io.BytesIO()\n",
    "while True:\n",
    "   s = stream_data.read(16384)\n",
    "   if not s:\n",
    "       break\n",
    "   tmp.write(s)\n",
    "stream_data.close()\n",
    "tmp.seek(0)\n",
    "# Extract tar file\n",
    "tar_file = tarfile.open(fileobj=tmp, mode=\"r:gz\")\n",
    "pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
    "neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
    "# Save pos/neg reviews (Also deal with encoding)\n",
    "pos_data = []\n",
    "for line in pos:\n",
    "   pos_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "neg_data = []\n",
    "for line in neg:\n",
    "   neg_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "tar_file.close()\n",
    "print(len(pos_data))\n",
    "print(len(neg_data))\n",
    "# Print out first negative review\n",
    "print(neg_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR-10 image data:** The Canadian Institute For Advanced Research has released an image set that contains 80 million labeled colored images (each image is scaled to 32x32 pixels). There are 10 different target classes (airplane, automobile, bird, and so on). The CIFAR-10 is a subset that has 60,000 images. There are 50,000 images in the training set, and 10,000 in the test set. Since we will be using this dataset in multiple ways, and because it is one of our larger datasets, we will not run a script each time we need it. To get this dataset, please navigate to http://www. cs.toronto.edu/~kriz/cifar.html, and download the CIFAR-10 dataset. We will address how to use this dataset in the appropriate chapters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The works of Shakespeare text data:** Project Gutenberg (5) is a project that releases electronic versions of free books. They have compiled all of the works of Shakespeare together. The following code shows you how to access this text file through Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5582212\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt' \n",
    "# Get Shakespeare text \n",
    "response = requests.get(shakespeare_url) \n",
    "shakespeare_file = response.content \n",
    "# Decode binary into string \n",
    "shakespeare_text = shakespeare_file.decode('utf-8') \n",
    "# Drop first few descriptive paragraphs. \n",
    "shakespeare_text = shakespeare_text[7675:] \n",
    "print(len(shakespeare_text)) # Number of characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**English-German sentence translation data:** The Tatoeba project (http://tatoeba.org) collects sentence translations in many languages. Their data has been released under the Creative Commons License. From this data, ManyThings.org (http://www.manythings.org) has compiled sentence-to-sentence translations in text files that are available for download. Here, we will use the English-German translation file, but you can change the URL to whichever languages you would like to use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176692\n",
      "176692\n",
      "['Go on.', 'Mach weiter.']\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import io \n",
    "from zipfile import ZipFile \n",
    "sentence_url = 'http://www.manythings.org/anki/deu-eng.zip' \n",
    "r = requests.get(sentence_url) \n",
    "z = ZipFile(io.BytesIO(r.content)) \n",
    "file = z.read('deu.txt') \n",
    "# Format Data \n",
    "eng_ger_data = file.decode() \n",
    "eng_ger_data = eng_ger_data.encode('ascii',errors='ignore') \n",
    "eng_ger_data = eng_ger_data.decode().split('\\n') \n",
    "eng_ger_data = [x.split('\\t') for x in eng_ger_data if len(x)>=1] \n",
    "[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)] \n",
    "print(len(english_sentence)) \n",
    "print(len(german_sentence)) \n",
    "print(eng_ger_data[10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
